Based on the provided TL code, translate it into CuTe NVIDIA language. Focus on generating functionally correct CuTe code that precisely reflects the memory operations and computational logic defined in the TL. Adhere to CuTe's syntax and best practices for memory management and kernel implementation.

Here's a brief overview of CuTe's relevance:

    CuTe: CuTe (CUDA Template Expressions) is a library provided by NVIDIA for writing high-performance CUDA kernels. It offers powerful abstractions for multi-dimensional array access, layout transformations, and memory operations, enabling developers to write expressive and efficient GPU code. CuTe is particularly well-suited for implementing complex memory access patterns and tensor operations, making it ideal for translating the concepts expressed in the TL.

The translation should primarily involve mapping the TL statements to their equivalent CuTe constructs. Consider the following when translating:

    Copy statements: Translate Copy statements into CuTe's memory transfer operations. This will involve defining appropriate Layout objects for source and destination memories (global, shared, register) and using CuTe's copy or similar functions to perform the data movement. Pay attention to offset and coordinate information for precise memory access.
    Allocate statements: These TL statements imply memory allocation and layout definitions. In CuTe, this translates to defining appropriate Layout objects for tensors in different memory spaces and potentially allocating memory using CUDA's memory management functions if explicit allocation is required (though often, CuTe's power comes from its ability to describe views into existing memory).
    Compute statements: Map Compute statements to CuTe's tensor operations or calls to specialized CUDA libraries (like CUTLASS for GEMM) if they align with the described computation. For generic arithmetic, direct CuTe tensor operations or standard operations on CuTe views should be used.
    Reshape statements: Translate Reshape statements into CuTe's Layout transformations. CuTe provides powerful mechanisms to reshape and restride tensors without actual data movement, which perfectly aligns with the intent of the Reshape statement in the TL.
    Loop statements: Translate for loops into standard for loops within the CUDA kernel, utilizing CuTe's indexing mechanisms to correctly iterate over tensor elements or blocks.

Ensure that the translated CuTe code is self-contained and runnable within a CUDA environment, assuming necessary boilerplate (kernel definition, launch configuration) is in place (you do not need to generate the full CUDA program, just the relevant kernel logic based on the TL). Assume standard CuTe includes and namespaces are in use.

Your output should be the CuTe DSL in python code corresponding to the provided TL.

Here are some examples on how to write CuTe DSL in Python:
Your First Program with CuTe DSL
Introduction
Welcome! In this tutorial, we'll write a simple "Hello World" program that runs on your GPU using CuTe DSL. This will help you understand the basics of GPU programming with our framework.

What You'll Learn
How to write code that runs on both CPU (host) and GPU (device),
How to launch a GPU kernel (a function that runs on the GPU),
Basic CUDA concepts like threads and thread blocks,
Step 1: Import Required Libraries
First, let's import the libraries we need:

import cutlass               
import cutlass.cute as cute  
Step 2: Write Our GPU Kernel
A GPU kernel is a function that runs on the GPU. Here's a simple kernel that prints "Hello World". Key concepts:

@cute.kernel: This decorator tells CUTLASS that this function should run on the GPU
cute.arch.thread_idx(): Gets the ID of the current GPU thread (like a worker's ID number)
We only want one thread to print the message (thread 0) to avoid multiple prints
@cute.kernel
def kernel():
    # Get the x component of the thread index (y and z components are unused)
    tidx, _, _ = cute.arch.thread_idx()
    # Only the first thread (thread 0) prints the message
    if tidx == 0:
        cute.printf("Hello world")
Step 3: Write Our Host Function
Now we need a function that sets up the GPU and launches our kernel. Key concepts:

@cute.jit: This decorator is for functions that run on the CPU but can launch GPU code
We need to initialize CUDA before using the GPU
.launch() tells CUDA how many blocks, threads, shared memory, etc. to use
@cute.jit
def hello_world():

    # Print hello world from host code
    cute.printf("hello world")

    # Launch kernel
    kernel().launch(
        grid=(1, 1, 1),   # Single thread block
        block=(32, 1, 1)  # One warp (32 threads) per thread block
    )
Step 4: Run Our Program
There are 2 ways we can run our program:

compile and run immediately
separate compilation which allows us to compile the code once and run multiple times
Please note the Compiling... for Method 2 prints before the "Hello world" of the first kernel. This shows the asynchronous behavior between CPU and GPU prints.

# Initialize CUDA context for launching a kernel with error checking
# We make context initialization explicit to allow users to control the context creation 
# and avoid potential issues with multiple contexts
cutlass.cuda.initialize_cuda_context()

# Method 1: Just-In-Time (JIT) compilation - compiles and runs the code immediately
print("Running hello_world()...")
hello_world()

# Method 2: Compile first (useful if you want to run the same code multiple times)
print("Compiling...")
hello_world_compiled = cute.compile(hello_world)

# Run the pre-compiled version
print("Running compiled version...")
hello_world_compiled()
Running hello_world()...
hello world
Compiling...
Hello world
Running compiled version...
hello world

====================================================================================

Printing with CuTe DSL
This notebook demonstrates the different ways to print values in CuTe and explains the important distinction between static (compile-time) and dynamic (runtime) values.

Key Concepts
Static values: Known at compile time
Dynamic values: Only known at runtime
Different printing methods for different scenarios
Layout representation in CuTe
Tensor visualization and formatting
import cutlass
import cutlass.cute as cute
import numpy as np
Print Example Function
The print_example function demonstrates several important concepts:

1. Python's print vs CuTe's cute.printf
print: Can only show static values at compile time
cute.printf: Can display both static and dynamic values at runtime
2. Value Types
a: Dynamic Int32 value (runtime)
b: Static Constexpr[int] value (compile-time)
3. Layout Printing
Shows how layouts are represented differently in static vs dynamic contexts:

Static context: Unknown values shown as ?
Dynamic context: Actual values displayed
@cute.jit
def print_example(a: cutlass.Int32, b: cutlass.Constexpr[int]):
    """
    Demonstrates different printing methods in CuTe and how they handle static vs dynamic values.

    This example shows:
    1. How Python's `print` function works with static values at compile time but can't show dynamic values
    2. How `cute.printf` can display both static and dynamic values at runtime
    3. The difference between types in static vs dynamic contexts
    4. How layouts are represented in both printing methods

    Args:
        a: A dynamic Int32 value that will be determined at runtime
        b: A static (compile-time constant) integer value
    """
    # Use Python `print` to print static information
    print(">>>", b)  # => 2
    # `a` is dynamic value
    print(">>>", a)  # => ?

    # Use `cute.printf` to print dynamic information
    cute.printf(">?? {}", a)  # => 8
    cute.printf(">?? {}", b)  # => 2

    print(">>>", type(a))  # => <class 'cutlass.Int32'>
    print(">>>", type(b))  # => <class 'int'>

    layout = cute.make_layout((a, b))
    print(">>>", layout)            # => (?,2):(1,?)
    cute.printf(">?? {}", layout)   # => (8,2):(1,8)
Compile and Run
Direct Compilation and Run

print_example(cutlass.Int32(8), 2)
Compiles and runs in one step will execute both static and dynamic print
>>> stands for static print
>?? stands for dynamic print
print_example(cutlass.Int32(8), 2)
>>> 2
>>> ?
>>> Int32
>>> <class 'int'>
>>> (?,2):(1,?)
>?? 8
>?? 2
>?? (8,2):(1,8)
Compile Function
When compiles the function with cute.compile(print_example, cutlass.Int32(8), 2), Python interpreter traces code and only evaluate static expression and print static information.

print_example_compiled = cute.compile(print_example, cutlass.Int32(8), 2)
>>> 2
>>> ?
>>> Int32
>>> <class 'int'>
>>> (?,2):(1,?)
Call compiled function
Only print out runtime information

print_example_compiled(cutlass.Int32(8))
>?? 8
>?? 2
>?? (8,2):(1,8)
Format String Example
The format_string_example function shows an important limitation:

F-strings in CuTe are evaluated at compile time
This means dynamic values won't show their runtime values in f-strings
Use cute.printf when you need to see runtime values
@cute.jit
def format_string_example(a: cutlass.Int32, b: cutlass.Constexpr[int]):
    """
    Format string is evaluated at compile time.
    """
    print(f"a: {a}, b: {b}")

    layout = cute.make_layout((a, b))
    print(f"layout: {layout}")

print("Direct run output:")
format_string_example(cutlass.Int32(8), 2)
Direct run output:
a: ?, b: 2
layout: (?,2):(1,?)
Printing Tensor Examples
CuTe provides specialized functionality for printing tensors through the print_tensor operation. The cute.print_tensor takes the following parameter:

Tensor (required): A CuTe tensor object that you want to print. The tensor must support load and store operations
verbose (optional, default=False): A boolean flag that controls the level of detail in the output. When set to True, it will print indices details for each element in the tensor.
Below example code shows the difference between verbose ON and OFF, and how to print a sub range of the given tensor.

from cutlass.cute.runtime import from_dlpack

@cute.jit
def print_tensor_basic(x : cute.Tensor):
    # Print the tensor
    print("Basic output:")
    cute.print_tensor(x)
    
@cute.jit
def print_tensor_verbose(x : cute.Tensor):
    # Print the tensor with verbose mode
    print("Verbose output:")
    cute.print_tensor(x, verbose=True)

@cute.jit
def print_tensor_slice(x : cute.Tensor, coord : tuple):
    # slice a 2D tensor from the 3D tensor
    sliced_data = cute.slice_(x, coord)
    y = cute.make_fragment(sliced_data.layout, sliced_data.element_type)
    # Convert to TensorSSA format by loading the sliced data into the fragment
    y.store(sliced_data.load())
    print("Slice output:")
    cute.print_tensor(y)
The default cute.print_tensor will output CuTe tensor with datatype, storage space, CuTe layout information, and print data in torch-style format.

def tensor_print_example1():
    shape = (4, 3, 2)
    
    # Creates [0,...,23] and reshape to (4, 3, 2)
    data = np.arange(24, dtype=np.float32).reshape(*shape) 
      
    print_tensor_basic(from_dlpack(data))

tensor_print_example1()
Basic output:
tensor(raw_ptr(0x000000000a5f1d50: f32, generic, align<4>) o (4,3,2):(6,2,1), data=
       [[[ 0.000000,  2.000000,  4.000000, ],
         [ 6.000000,  8.000000,  10.000000, ],
         [ 12.000000,  14.000000,  16.000000, ],
         [ 18.000000,  20.000000,  22.000000, ]],

        [[ 1.000000,  3.000000,  5.000000, ],
         [ 7.000000,  9.000000,  11.000000, ],
         [ 13.000000,  15.000000,  17.000000, ],
         [ 19.000000,  21.000000,  23.000000, ]]])
The verbosed print will show coodination details of each element in the tensor. The below example shows how we index element in a 2D 4x3 tensor space.

def tensor_print_example2():
    shape = (4, 3)
    
    # Creates [0,...,11] and reshape to (4, 3)
    data = np.arange(12, dtype=np.float32).reshape(*shape) 
      
    print_tensor_verbose(from_dlpack(data))

tensor_print_example2()
Verbose output:
tensor(raw_ptr(0x000000000a814cc0: f32, generic, align<4>) o (4,3):(3,1), data= (
	(0,0)= 0.000000
	(0,1)= 1.000000
	(0,2)= 2.000000
	(1,0)= 3.000000
	(1,1)= 4.000000
	(1,2)= 5.000000
	(2,0)= 6.000000
	(2,1)= 7.000000
	(2,2)= 8.000000
	(3,0)= 9.000000
	(3,1)= 10.000000
	(3,2)= 11.000000
)
To print a subset elements in the given Tensor, we can use cute.slice_ to select a range of the given tensor, load them into register and then print the values with cute.print_tensor.

def tensor_print_example3():
    shape = (4, 3)
    
    # Creates [0,...,11] and reshape to (4, 3)
    data = np.arange(12, dtype=np.float32).reshape(*shape) 
      
    print_tensor_slice(from_dlpack(data), (None, 0))
    print_tensor_slice(from_dlpack(data), (1, None))

tensor_print_example3()
Slice output:
tensor(raw_ptr(0x00007ffeeae1fc60: f32, rmem, align<32>) o (4):(3), data=
       [ 0.000000, ],
       [ 3.000000, ],
       [Slice output:
 6.000000, ],
       [ 9.000000, ])
tensor(raw_ptr(0x00007ffeeae1fc60: f32, rmem, align<32>) o (3):(1), data=
       [ 3.000000, ],
       [ 4.000000, ],
       [ 5.000000, ])

===================================================================================

from typing import List

import cutlass
import cutlass.cute as cute
Understanding data structure in CuTe DSL
In most cases, data structures in CuTe DSL work the same as Python data structures with the notable difference that Python data structures in most cases are considered as static data which are interpreted by the DSL compiler embedded inside Python interpreter.

To differentiate between compile-time and runtime values, CuTe DSL introduces primitive types that represent dynamic values in JIT-compiled code.

CuTe DSL provides a comprehensive set of primitive numeric types for representing dynamic values at runtime. These types are formally defined within the CuTe DSL typing system:

Integer Types
Int8 - 8-bit signed integer
Int16 - 16-bit signed integer
Int32 - 32-bit signed integer
Int64 - 64-bit signed integer
Int128 - 128-bit signed integer
Uint8 - 8-bit unsigned integer
Uint16 - 16-bit unsigned integer
Uint32 - 32-bit unsigned integer
Uint64 - 64-bit unsigned integer
Uint128 - 128-bit unsigned integer
Floating Point Types
Float16 - 16-bit floating point
Float32 - 32-bit floating point
Float64 - 64-bit floating point
BFloat16 - Brain Floating Point format (16-bit)
TFloat32 - Tensor Float32 format (reduced precision format used in tensor operations)
Float8E4M3 - 8-bit floating point with 4-bit exponent and 3-bit mantissa
Float8E5M2 - 8-bit floating point with 5-bit exponent and 2-bit mantissa
These specialized types are designed to represent dynamic values in CuTe DSL code that will be evaluated at runtime, in contrast to Python's built-in numeric types which are evaluated during compilation.

Example usage:
x = cutlass.Int32(5)        # Creates a 32-bit integer
y = cutlass.Float32(3.14)   # Creates a 32-bit float

@cute.jit
def foo(a: cutlass.Int32):  # annotate `a` as 32-bit integer passed to jit function via ABI
    ...
To differentiate between compile-time and runtime values, CuTe DSL introduces primitive types that represent dynamic values in JIT-compiled code.

CuTe DSL provides a comprehensive set of primitive numeric types for representing dynamic values at runtime. These types are formally defined within the CuTe DSL typing system:

Integer Types
Int8 - 8-bit signed integer
Int16 - 16-bit signed integer
Int32 - 32-bit signed integer
Int64 - 64-bit signed integer
Int128 - 128-bit signed integer
Uint8 - 8-bit unsigned integer
Uint16 - 16-bit unsigned integer
Uint32 - 32-bit unsigned integer
Uint64 - 64-bit unsigned integer
Uint128 - 128-bit unsigned integer
Floating Point Types
Float16 - 16-bit floating point
Float32 - 32-bit floating point
Float64 - 64-bit floating point
BFloat16 - Brain Floating Point format (16-bit)
TFloat32 - Tensor Float32 format (reduced precision format used in tensor operations)
Float8E4M3 - 8-bit floating point with 4-bit exponent and 3-bit mantissa
Float8E5M2 - 8-bit floating point with 5-bit exponent and 2-bit mantissa
These specialized types are designed to represent dynamic values in CuTe DSL code that will be evaluated at runtime, in contrast to Python's built-in numeric types which are evaluated during compilation.

Example usage:
x = cutlass.Int32(5)        # Creates a 32-bit integer
y = cutlass.Float32(3.14)   # Creates a 32-bit float

@cute.jit
def foo(a: cutlass.Int32):  # annotate `a` as 32-bit integer passed to jit function via ABI
    ...
@cute.jit
def bar():
    a = cutlass.Float32(3.14)
    print("a(static) =", a)             # prints `a(static) = ?`
    cute.printf("a(dynamic) = {}", a)   # prints `a(dynamic) = 3.140000`

    b = cutlass.Int32(5)
    print("b(static) =", b)             # prints `b(static) = 5`
    cute.printf("b(dynamic) = {}", b)   # prints `b(dynamic) = 5`

bar()
a(static) = ?
b(static) = ?
a(dynamic) = 3.140000
b(dynamic) = 5
Type Conversion API
CUTLASS numeric types provide type conversion through the to() method available on all Numeric types. This allows you to convert between different numeric data types at runtime.

Syntax:

new_value = value.to(target_type)
The to() method supports conversion between:

Integer types (Int8, Int16, Int32, Int64, UInt8, UInt16, UInt32, UInt64)
Floating point types (Float16, Float32, Float64, BFloat16)
Mixed integer/floating point conversions
Note that when converting from floating point to integer types, the decimal portion is truncated. When converting between types with different ranges, values may be clamped or lose precision if they exceed the target type's representable range.

@cute.jit
def type_conversion():
    # Convert from Int32 to Float32
    x = cutlass.Int32(42)
    y = x.to(cutlass.Float32)
    cute.printf("Int32({}) => Float32({})", x, y)

    # Convert from Float32 to Int32
    a = cutlass.Float32(3.14)
    b = a.to(cutlass.Int32)
    cute.printf("Float32({}) => Int32({})", a, b)

    # Convert from Int32 to Int8
    c = cutlass.Int32(127)
    d = c.to(cutlass.Int8)
    cute.printf("Int32({}) => Int8({})", c, d)

    # Convert from Int32 to Int8 with value exceeding Int8 range
    e = cutlass.Int32(300)
    f = e.to(cutlass.Int8)
    cute.printf("Int32({}) => Int8({}) (truncated due to range limitation)", e, f)

type_conversion()
Int32(42) => Float32(42.000000)
Float32(3.140000) => Int32(3)
Int32(127) => Int8(127)
Int32(300) => Int8(44) (truncated due to range limitation)
Operator Overloading
CUTLASS numeric types support Python's built-in operators, allowing you to write natural mathematical expressions. The operators work with both CUTLASS numeric types and Python native numeric types.

Supported operators include:

Arithmetic: +, -, *, /, //, %, **
Comparison: <, <=, ==, !=, >=, >
Bitwise: &, |, ^, <<, >>
Unary: - (negation), ~ (bitwise NOT)
@cute.jit
def operator_demo():
    # Arithmetic operators
    a = cutlass.Int32(10)
    b = cutlass.Int32(3)
    cute.printf("a: Int32({}), b: Int32({})", a, b)

    x = cutlass.Float32(5.5)
    cute.printf("x: Float32({})", x)

    cute.printf("")

    sum_result = a + b
    cute.printf("a + b = {}", sum_result)

    y = x * 2  # Multiplying with Python native type
    cute.printf("x * 2 = {}", y)

    # Mixed type arithmetic (Int32 + Float32) that integer is converted into float32
    mixed_result = a + x
    cute.printf("a + x = {} (Int32 + Float32 promotes to Float32)", mixed_result)

    # Division with Int32 (note: integer division)
    div_result = a / b
    cute.printf("a / b = {}", div_result)

    # Float division
    float_div = x / cutlass.Float32(2.0)
    cute.printf("x / 2.0 = {}", float_div)

    # Comparison operators
    is_greater = a > b
    cute.printf("a > b = {}", is_greater)

    # Bitwise operators
    bit_and = a & b
    cute.printf("a & b = {}", bit_and)

    neg_a = -a
    cute.printf("-a = {}", neg_a)

    not_a = ~a
    cute.printf("~a = {}", not_a)

operator_demo()
a: Int32(10), b: Int32(3)
x: Float32(5.500000)

a + b = 13
x * 2 = 11.000000
a + x = 15.500000 (Int32 + Float32 promotes to Float32)
a / b = 3.333333
x / 2.0 = 2.750000
a > b = 1
a & b = 2
-a = -10
~a = -11

==============================================================================

import cutlass
import cutlass.cute as cute
Tensor
A tensor in CuTe is created through the composition of two key components:

An Engine (E) - A random-access, pointer-like object that supports:

Offset operation: e + d → e (offset engine by elements of a layout's codomain)
Dereference operation: *e → v (dereference engine to produce value)
A Layout (L) - Defines the mapping from coordinates to offsets

A tensor is formally defined as the composition of an engine E with a layout L, expressed as T = E ∘ L. When evaluating a tensor at coordinate c, it:

Maps the coordinate c to the codomain using the layout
Offsets the engine accordingly
Dereferences the result to obtain the tensor's value
This can be expressed mathematically as:

T(c) = (E ∘ L)(c) = *(E + L(c))
Example Usage
Here's a simple example of creating a tensor using pointer and layout (8,5):(5,1) and fill with ones:

@cute.jit
def create_tensor_from_ptr(ptr: cute.Pointer):
    layout = cute.make_layout((8, 5), stride=(5, 1))
    tensor = cute.make_tensor(ptr, layout)
    tensor.fill(1)
    cute.print_tensor(tensor)
This creates a tensor where:

The engine is a pointer
The layout with shape (8, 5) and stride (5, 1)
The resulting tensor can be evaluated using coordinates defined by the layout
We can test this by allocating buffer with torch and run test with pointer to torch tensor

import torch

from cutlass.torch import dtype as torch_dtype
import cutlass.cute.runtime as cute_rt

a = torch.randn(8, 5, dtype=torch_dtype(cutlass.Float32))
ptr_a = cute_rt.make_ptr(cutlass.Float32, a.data_ptr())

create_tensor_from_ptr(ptr_a)
tensor(raw_ptr(0x000000000736b0c0: f32, generic, align<4>) o (8,5):(5,1), data=
       [[ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ],
        [ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ],
        [ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ],
        ...
        [ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ],
        [ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ],
        [ 1.000000,  1.000000,  1.000000,  1.000000,  1.000000, ]])
DLPACK support
CuTe DSL is designed to support dlpack protocol natively. This offers easy integration with frameworks supporting DLPack, e.g. torch, numpy, jax, tensorflow, etc.

For more information, please refer to DLPACK project: https://github.com/dmlc/dlpack

Calling from_dlpack can convert any tensor or ndarray object supporting __dlpack__ and __dlpack_device__.

from cutlass.cute.runtime import from_dlpack

@cute.jit
def print_tensor_dlpack(src: cute.Tensor):
    print(src)
    cute.print_tensor(src)
a = torch.randn(8, 5, dtype=torch_dtype(cutlass.Float32))

print_tensor_dlpack(from_dlpack(a))
tensor<ptr<f32, generic> o (8,5):(5,1)>
tensor(raw_ptr(0x0000000007559340: f32, generic, align<4>) o (8,5):(5,1), data=
       [[-1.151769,  1.019397, -0.371175, -0.717776,  0.502176, ],
        [ 0.114282,  0.900084,  0.320770,  1.564574, -0.632329, ],
        [-0.570140,  0.178112, -0.423079,  1.936198,  0.003355, ],
        ...
        [-2.425393, -0.275528,  1.267157, -0.811101, -0.985456, ],
        [ 0.777889, -2.114074,  0.357184, -0.321312, -0.938138, ],
        [ 1.959564,  1.797602,  0.116901,  0.306198, -1.837295, ]])
import numpy as np

a = np.random.randn(8, 8).astype(np.float32)

print_tensor_dlpack(from_dlpack(a))
tensor<ptr<f32, generic> o (8,8):(8,1)>
tensor(raw_ptr(0x0000000007979da0: f32, generic, align<4>) o (8,8):(8,1), data=
       [[ 0.122739, -0.605744, -1.442022, ..., -0.356501, -0.993329, -0.091110, ],
        [ 0.278448,  0.318482, -0.276867, ...,  1.542181, -1.701539, -0.309454, ],
        [ 0.563565, -0.753936,  0.131214, ...,  0.437912, -0.482277, -0.051540, ],
        ...
        [-1.974096, -0.177881,  0.426807, ..., -1.579115, -0.304974,  0.451164, ],
        [ 0.149851, -0.704689, -0.295063, ..., -0.653001,  0.008871,  0.903916, ],
        [ 1.188619,  1.519662,  1.270734, ...,  0.404082,  0.173200,  0.093476, ]])
Tensor Evaluation Methods
Tensors support two primary methods of evaluation:

1. Full Evaluation
When applying the tensor evaluation with a complete coordinate c, it computes the offset, applies it to the engine, and dereferences it to return the stored value. This is the straightforward case where you want to access a specific element of the tensor.

2. Partial Evaluation (Slicing)
When evaluating with an incomplete coordinate c = c' ⊕ c* (where c* represents the unspecified portion), the result is a new tensor which is a slice of the original tensor with its engine offset to account for the coordinates that were provided. This operation can be expressed as:

T(c) = (E ∘ L)(c) = (E + L(c')) ∘ L(c*) = T'(c*)
Slicing effectively reduces the dimensionality of the tensor, creating a sub-tensor that can be further evaluated or manipulated.

@cute.jit
def tensor_access_item(a: cute.Tensor):
    # access data using linear index
    cute.printf("a[2] = {} (equivalent to a[{}])", a[2],
                cute.make_identity_tensor(a.layout.shape)[2])
    cute.printf("a[9] = {} (equivalent to a[{}])", a[9],
                cute.make_identity_tensor(a.layout.shape)[9])

    # access data using n-d coordinates, following two are equivalent
    cute.printf("a[2,0] = {}", a[2, 0])
    cute.printf("a[2,4] = {}", a[2, 4])
    cute.printf("a[(2,4)] = {}", a[2, 4])

    # assign value to tensor@(2,4)
    a[2,3] = 100.0
    a[2,4] = 101.0
    cute.printf("a[2,3] = {}", a[2,3])
    cute.printf("a[(2,4)] = {}", a[(2,4)])

@cute.kernel
def print_tensor_gpu(ptr: cute.Pointer):
    layout = cute.make_layout((8, 5), stride=(5, 1))
    tensor = cute.make_tensor(ptr, layout)

    tidx, _, _ = cute.arch.thread_idx()

    if tidx == 0:
        cute.print_tensor(tensor)


# Create a tensor with sequential data using torch
data = torch.arange(0, 8*5, dtype=torch.float32).reshape(8, 5)
tensor_access_item(from_dlpack(data))

print(data)
a[2] = 10.000000 (equivalent to a[(2,0)])
a[9] = 6.000000 (equivalent to a[(1,1)])
a[2,0] = 10.000000
a[2,4] = 14.000000
a[(2,4)] = 14.000000
a[2,3] = 100.000000
a[(2,4)] = 101.000000
tensor([[  0.,   1.,   2.,   3.,   4.],
        [  5.,   6.,   7.,   8.,   9.],
        [ 10.,  11.,  12., 100., 101.],
        [ 15.,  16.,  17.,  18.,  19.],
        [ 20.,  21.,  22.,  23.,  24.],
        [ 25.,  26.,  27.,  28.,  29.],
        [ 30.,  31.,  32.,  33.,  34.],
        [ 35.,  36.,  37.,  38.,  39.]])
Tensor as memory view
In CUDA programming, different memory spaces have different characteristics in terms of access speed, scope, and lifetime:

generic: Default memory space that can refer to any other memory space.
global memory (gmem): Accessible by all threads across all blocks, but has higher latency.
shared memory (smem): Accessible by all threads within a block, with much lower latency than global memory.
register memory (rmem): Thread-private memory with the lowest latency, but limited capacity.
tensor memory (tmem): Specialized memory introduced in NVIDIA Blackwell architecture for tensor operations.
When creating tensors in CuTe, you can specify the memory space to optimize performance based on your access patterns.

For more information on CUDA memory spaces, see the CUDA Programming Guide.

Coordinate Tensor
A coordinate tensor is a special type of tensor that maps coordinates to coordinates rather than to values. The key distinction is that while regular tensors map coordinates to some value type (like numbers), coordinate tensors map coordinates to other coordinates.

Each element in the tensor is itself a coordinate tuple (i,j) rather than a scalar value
The coordinates map to themselves - so position (1,2) contains the coordinate (1,2)
The layout (row-major vs column-major) determines how these coordinate tuples are arranged in memory
For example, coordinate tensors can be created using the make_identity_tensor utility:

coord_tensor = make_identity_tensor(layout.shape())
This creates a tensor that maps each coordinate to itself, providing a reference point for understanding how other layouts transform these coordinates.

@cute.jit
def print_tensor_coord(a: cute.Tensor):
    coord_tensor = cute.make_identity_tensor(a.layout.shape)
    print(coord_tensor)

a = torch.randn(8,4, dtype=torch_dtype(cutlass.Float32))
print_tensor_coord(from_dlpack(a))

============================================================================

import cutlass
import cutlass.cute as cute
from cutlass.cute.runtime import from_dlpack

import numpy as np
import torch
Introduction to the TensorSSA in CuTe DSL
This tutorial introduces what is the TensorSSA and why we need it. We also give some examples to show how to use TensorSSA.

What is TensorSSA
TensorSSA is a Python class that represents a tensor value in Static Single Assignment (SSA) form within the CuTe DSL. You can think of it as a tensor residing in a (simulated) register.

Why TensorSSA
TensorSSA encapsulates the underlying MLIR tensor value into an object that's easier to manipulate in Python. By overloading numerous Python operators (like +, -, *, /, [], etc.), it allows users to express tensor computations (primarily element-wise operations and reductions) in a more Pythonic way. These element-wise operations are then translated into optimized vectorization instructions.

It's part of the CuTe DSL, serving as a bridge between the user-described computational logic and the lower-level MLIR IR, particularly for representing and manipulating register-level data.

When to use TensorSSA
TensorSSA is primarily used in the following scenarios:

Load from memory and store to memory
@cute.jit
def load_and_store(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):
    """
    Load data from memory and store the result to memory.

    :param res: The destination tensor to store the result.
    :param a: The source tensor to be loaded.
    :param b: The source tensor to be loaded.
    """
    a_vec = a.load()
    print(f"a_vec: {a_vec}")      # prints `a_vec: vector<12xf32> o (3, 4)`
    b_vec = b.load()
    print(f"b_vec: {b_vec}")      # prints `b_vec: vector<12xf32> o (3, 4)`
    res.store(a_vec + b_vec)
    cute.print_tensor(res)

a = np.ones(12).reshape((3, 4)).astype(np.float32)
b = np.ones(12).reshape((3, 4)).astype(np.float32)
c = np.zeros(12).reshape((3, 4)).astype(np.float32)
load_and_store(from_dlpack(c), from_dlpack(a), from_dlpack(b))
a_vec: tensor_value<vector<12xf32> o (3, 4)>
b_vec: tensor_value<vector<12xf32> o (3, 4)>
tensor(raw_ptr(0x0000000006cff170: f32, generic, align<4>) o (3,4):(4,1), data=
       [[ 2.000000,  2.000000,  2.000000,  2.000000, ],
        [ 2.000000,  2.000000,  2.000000,  2.000000, ],
        [ 2.000000,  2.000000,  2.000000,  2.000000, ]])
Register-Level Tensor Operations
When writing kernel logic, various computations, transformations, slicing, etc., are performed on data loaded into registers.

@cute.jit
def apply_slice(src: cute.Tensor, dst: cute.Tensor, indices: cutlass.Constexpr):
    """
    Apply slice operation on the src tensor and store the result to the dst tensor.

    :param src: The source tensor to be sliced.
    :param dst: The destination tensor to store the result.
    :param indices: The indices to slice the source tensor.
    """
    src_vec = src.load()
    dst_vec = src_vec[indices]
    print(f"{src_vec} -> {dst_vec}")
    if isinstance(dst_vec, cute.TensorSSA):
        dst.store(dst_vec)
        cute.print_tensor(dst)
    else:
        dst[0] = dst_vec
        cute.print_tensor(dst)

def slice_1():
    src_shape = (4, 2, 3)
    dst_shape = (4, 3)
    indices = (None, 1, None)

    """
    a:
    [[[ 0.  1.  2.]
      [ 3.  4.  5.]]

     [[ 6.  7.  8.]
      [ 9. 10. 11.]]

     [[12. 13. 14.]
      [15. 16. 17.]]

     [[18. 19. 20.]
      [21. 22. 23.]]]
    """
    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)
    dst = np.random.randn(*dst_shape).astype(np.float32)
    apply_slice(from_dlpack(a), from_dlpack(dst), indices)

slice_1()
tensor_value<vector<24xf32> o (4, 2, 3)> -> tensor_value<vector<12xf32> o (4, 3)>
tensor(raw_ptr(0x00000000071acaf0: f32, generic, align<4>) o (4,3):(3,1), data=
       [[ 3.000000,  4.000000,  5.000000, ],
        [ 9.000000,  10.000000,  11.000000, ],
        [ 15.000000,  16.000000,  17.000000, ],
        [ 21.000000,  22.000000,  23.000000, ]])
def slice_2():
    src_shape = (4, 2, 3)
    dst_shape = (1,)
    indices = 10
    a = np.arange(np.prod(src_shape)).reshape(*src_shape).astype(np.float32)
    dst = np.random.randn(*dst_shape).astype(np.float32)
    apply_slice(from_dlpack(a), from_dlpack(dst), indices)

slice_2()
tensor_value<vector<24xf32> o (4, 2, 3)> -> ?
tensor(raw_ptr(0x00000000013cbbe0: f32, generic, align<4>) o (1):(1), data=
       [ 10.000000, ])
Arithmetic Operations
As we mentioned earlier, there're many tensor operations whose operands are TensorSSA. And they are all element-wise operations. We give some examples below.

Binary Operations
For binary operations, the LHS operand is TensorSSA and the RHS operand can be either TensorSSA or Numeric. When the RHS is Numeric, it will be broadcast to a TensorSSA.

@cute.jit
def binary_op_1(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):
    a_vec = a.load()
    b_vec = b.load()

    add_res = a_vec + b_vec
    res.store(add_res)
    cute.print_tensor(res)        # prints [3.000000, 3.000000, 3.000000]

    sub_res = a_vec - b_vec
    res.store(sub_res)
    cute.print_tensor(res)        # prints [-1.000000, -1.000000, -1.000000]

    mul_res = a_vec * b_vec
    res.store(mul_res)
    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]

    div_res = a_vec / b_vec
    res.store(div_res)
    cute.print_tensor(res)        # prints [0.500000, 0.500000, 0.500000]

    floor_div_res = a_vec // b_vec
    res.store(floor_div_res)
    cute.print_tensor(res)        # prints [0.000000, 0.000000, 0.000000]

    mod_res = a_vec % b_vec
    res.store(mod_res)
    cute.print_tensor(res)        # prints [1.000000, 1.000000, 1.000000]


a = np.empty((3,), dtype=np.float32)
a.fill(1.0)
b = np.empty((3,), dtype=np.float32)
b.fill(2.0)
res = np.empty((3,), dtype=np.float32)
binary_op_1(from_dlpack(res), from_dlpack(a), from_dlpack(b))
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [ 3.000000, ],
       [ 3.000000, ],
       [ 3.000000, ])
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [-1.000000, ],
       [-1.000000, ],
       [-1.000000, ])
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [ 2.000000, ],
       [ 2.000000, ],
       [ 2.000000, ])
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [ 0.500000, ],
       [ 0.500000, ],
       [ 0.500000, ])
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [ 0.000000, ],
       [ 0.000000, ],
       [ 0.000000, ])
tensor(raw_ptr(0x00000000074f0e70: f32, generic, align<4>) o (3):(1), data=
       [ 1.000000, ],
       [ 1.000000, ],
       [ 1.000000, ])
@cute.jit
def binary_op_2(res: cute.Tensor, a: cute.Tensor, c: cutlass.Constexpr):
    a_vec = a.load()

    add_res = a_vec + c
    res.store(add_res)
    cute.print_tensor(res)        # prints [3.000000, 3.000000, 3.000000]

    sub_res = a_vec - c
    res.store(sub_res)
    cute.print_tensor(res)        # prints [-1.000000, -1.000000, -1.000000]

    mul_res = a_vec * c
    res.store(mul_res)
    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]

    div_res = a_vec / c
    res.store(div_res)
    cute.print_tensor(res)        # prints [0.500000, 0.500000, 0.500000]

    floor_div_res = a_vec // c
    res.store(floor_div_res)
    cute.print_tensor(res)        # prints [0.000000, 0.000000, 0.000000]

    mod_res = a_vec % c
    res.store(mod_res)
    cute.print_tensor(res)        # prints [1.000000, 1.000000, 1.000000]

a = np.empty((3,), dtype=np.float32)
a.fill(1.0)
c = 2.0
res = np.empty((3,), dtype=np.float32)
binary_op_2(from_dlpack(res), from_dlpack(a), c)
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [ 3.000000, ],
       [ 3.000000, ],
       [ 3.000000, ])
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [-1.000000, ],
       [-1.000000, ],
       [-1.000000, ])
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [ 2.000000, ],
       [ 2.000000, ],
       [ 2.000000, ])
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [ 0.500000, ],
       [ 0.500000, ],
       [ 0.500000, ])
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [ 0.000000, ],
       [ 0.000000, ],
       [ 0.000000, ])
tensor(raw_ptr(0x0000000007828ed0: f32, generic, align<4>) o (3):(1), data=
       [ 1.000000, ],
       [ 1.000000, ],
       [ 1.000000, ])
@cute.jit
def binary_op_3(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):
    a_vec = a.load()
    b_vec = b.load()

    gt_res = a_vec > b_vec
    res.store(gt_res)

    """
    ge_res = a_ >= b_   # [False, True, False]
    lt_res = a_ < b_    # [True, False, True]
    le_res = a_ <= b_   # [True, False, True]
    eq_res = a_ == b_   # [False, False, False]
    """

a = np.array([1, 2, 3], dtype=np.float32)
b = np.array([2, 1, 4], dtype=np.float32)
res = np.empty((3,), dtype=np.bool_)
binary_op_3(from_dlpack(res), from_dlpack(a), from_dlpack(b))
print(res)     # prints [False, True, False]
[False  True False]
@cute.jit
def binary_op_4(res: cute.Tensor, a: cute.Tensor, b: cute.Tensor):
    a_vec = a.load()
    b_vec = b.load()

    xor_res = a_vec ^ b_vec
    res.store(xor_res)

    # or_res = a_vec | b_vec
    # res.store(or_res)     # prints [3, 2, 7]

    # and_res = a_vec & b_vec
    # res.store(and_res)      # prints [0, 2, 0]

a = np.array([1, 2, 3], dtype=np.int32)
b = np.array([2, 2, 4], dtype=np.int32)
res = np.empty((3,), dtype=np.int32)
binary_op_4(from_dlpack(res), from_dlpack(a), from_dlpack(b))
print(res)     # prints [3, 0, 7]
[3 0 7]
Unary Operations
@cute.jit
def unary_op_1(res: cute.Tensor, a: cute.Tensor):
    a_vec = a.load()

    sqrt_res = cute.math.sqrt(a_vec)
    res.store(sqrt_res)
    cute.print_tensor(res)        # prints [2.000000, 2.000000, 2.000000]

    sin_res = cute.math.sin(a_vec)
    res.store(sin_res)
    cute.print_tensor(res)        # prints [-0.756802, -0.756802, -0.756802]

    exp2_res = cute.math.exp2(a_vec)
    res.store(exp2_res)
    cute.print_tensor(res)        # prints [16.000000, 16.000000, 16.000000]

a = np.array([4.0, 4.0, 4.0], dtype=np.float32)
res = np.empty((3,), dtype=np.float32)
unary_op_1(from_dlpack(res), from_dlpack(a))
tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=
       [ 2.000000, ],
       [ 2.000000, ],
       [ 2.000000, ])
tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=
       [-0.756802, ],
       [-0.756802, ],
       [-0.756802, ])
tensor(raw_ptr(0x0000000007fbd180: f32, generic, align<4>) o (3):(1), data=
       [ 16.000000, ],
       [ 16.000000, ],
       [ 16.000000, ])
Reduction Operation
The TensorSSA's reduce method applies a specified reduction operation (ReductionOp.ADD, ReductionOp.MUL, ReductionOp.MAX, ReductionOp.MIN) starting with an initial value, and performs this reduction along the dimensions specified by the reduction_profile.. The result is typically a new TensorSSA with reduced dimensions or a scalar value if reduces across all axes.

@cute.jit
def reduction_op(a: cute.Tensor):
    """
    Apply reduction operation on the src tensor.

    :param src: The source tensor to be reduced.
    """
    a_vec = a.load()
    red_res = a_vec.reduce(
        cute.ReductionOp.ADD,
        0.0,
        reduction_profile=0
    )
    cute.printf(red_res)        # prints 21.000000

    red_res = a_vec.reduce(
        cute.ReductionOp.ADD,
        0.0,
        reduction_profile=(None, 1)
    )
    # We can't print the TensorSSA directly at this point, so we store it to a new Tensor and print it.
    res = cute.make_fragment(red_res.shape, cutlass.Float32)
    res.store(red_res)
    cute.print_tensor(res)        # prints [6.000000, 15.000000]

    red_res = a_vec.reduce(
        cute.ReductionOp.ADD,
        1.0,
        reduction_profile=(1, None)
    )
    res = cute.make_fragment(red_res.shape, cutlass.Float32)
    res.store(red_res)
    cute.print_tensor(res)        # prints [6.000000, 8.000000, 10.000000]


a = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)
reduction_op(from_dlpack(a))
21.000000
tensor(raw_ptr(0x00007ffd1ea2bca0: f32, rmem, align<32>) o (2):(1), data=
       [ 6.000000, ],
       [ 15.000000, ])
tensor(raw_ptr(0x00007ffd1ea2bcc0: f32, rmem, align<32>) o (3):(1), data=
       [ 6.000000, ],
       [ 8.000000, ],
       [ 10.000000, ])

======================================================================================

Using CuTe Layout Algebra With Python DSL
Referencing the 01_layout.md and 02_layout_algebra.md documentation from CuTe C++, we summarize:

In CuTe, a Layout:

is defined by a pair of Shape and Stride,
maps coordinates space(s) to an index space,
supports both static (compile-time) and dynamic (runtime) values.
CuTe also provides a powerful set of operations—the Layout Algebra—for combining and manipulating layouts, including:

Layout composition: Functional composition of layouts,
Layout "divide": Splitting a layout into two component layouts,
Layout "product": Reproducing a layout according to another layout.
In this notebook, we will demonstrate:

How to use CuTe’s key layout algebra operations with the Python DSL.
How static and dynamic layouts behave when printed or manipulated within the Python DSL.
We use examples from 02_layout_algebra.md which we recommend to the reader for additional details.

import cutlass
import cutlass.cute as cute
Layout Algebra Operations
These operations form the foundation of CuTe's layout manipulation capabilities, enabling:

Efficient data tiling and partitioning,
Separation of thread and data layouts with a canonical type to represent both,
Native description and manipulation of hierarchical tensors of threads and data crucial for tensor core programs,
Mixed static/dynamic layout transformations,
Seamless integration of layout algebra with tensor operations,
Expression of complex MMA and copies as canonical loops.
1. Coalesce
The coalesce operation simplifies a layout by flattening and combining modes when possible, without changing its size or behavior as a function on the integers.

It ensures the post-conditions:

Preserve size: cute.size(layout) == cute.size(result),
Flattened: depth(result) <= 1,
Preserve functional: For all i, 0 <= i < cute.size(layout), layout(i) == result(i).
Examples
Basic Coalesce Example :
@cute.jit
def coalesce_example():
    """
    Demonstrates coalesce operation flattening and combining modes
    """
    layout = cute.make_layout((2, (1, 6)), stride=(1, (cutlass.Int32(6), 2))) # Dynamic stride
    result = cute.coalesce(layout)

    print(">>> Original:", layout)
    cute.printf(">?? Original: {}", layout)
    print(">>> Coalesced:", result)
    cute.printf(">?? Coalesced: {}", result)

coalesce_example()
>>> Original: (2,(1,6)):(1,(?,2))
>>> Coalesced: 12:1
>?? Original: (2,(1,6)):(1,(6,2))
>?? Coalesced: 12:1
@cute.jit
def coalesce_post_conditions():
    """
    Demonstrates coalesce operation's 3 post-conditions:
    1. size(@a result) == size(@a layout)
    2. depth(@a result) <= 1
    3. for all i, 0 <= i < size(@a layout), @a result(i) == @a layout(i)
    """
    layout = cute.make_layout(
        ((2, (3, 4)), (3, 2), 1),
        stride=((4, (8, 24)), (2, 6), 12)
    )
    result = cute.coalesce(layout)

    print(">>> Original:", layout)
    print(">>> Coalesced:", result)

    print(">>> Checking post-conditions:")
    print(">>> 1. Checking size remains the same after the coalesce operation:")
    original_size = cute.size(layout)
    coalesced_size = cute.size(result)
    print(f"Original size: {original_size}, Coalesced size: {coalesced_size}")
    assert coalesced_size == original_size, \
            f"Size mismatch: original {original_size}, coalesced {coalesced_size}"
    
    print(">>> 2. Checking depth of coalesced layout <= 1:")
    depth = cute.depth(result)
    print(f"Depth of coalesced layout: {depth}")
    assert depth <= 1, f"Depth of coalesced layout should be <= 1, got {depth}"

    print(">>> 3. Checking layout functionality remains the same after the coalesce operation:")
    for i in range(original_size):
        original_value = layout(i)
        coalesced_value = result(i)
        print(f"Index {i}: original {original_value}, coalesced {coalesced_value}")
        assert coalesced_value == original_value, \
            f"Value mismatch at index {i}: original {original_value}, coalesced {coalesced_value}"

coalesce_post_conditions()
>>> Original: ((2,(3,4)),(3,2),1):((4,(8,24)),(2,6),12)
>>> Coalesced: (24,6):(4,2)
>>> Checking post-conditions:
>>> 1. Checking size remains the same after the coalesce operation:
Original size: 144, Coalesced size: 144
>>> 2. Checking depth of coalesced layout <= 1:
Depth of coalesced layout: 1
>>> 3. Checking layout functionality remains the same after the coalesce operation:
Index 0: original 0, coalesced 0
Index 1: original 4, coalesced 4
Index 2: original 8, coalesced 8
Index 3: original 12, coalesced 12
Index 4: original 16, coalesced 16
Index 5: original 20, coalesced 20
Index 6: original 24, coalesced 24
Index 7: original 28, coalesced 28
Index 8: original 32, coalesced 32
Index 9: original 36, coalesced 36
Index 10: original 40, coalesced 40
Index 11: original 44, coalesced 44
Index 12: original 48, coalesced 48
Index 13: original 52, coalesced 52
Index 14: original 56, coalesced 56
Index 15: original 60, coalesced 60
Index 16: original 64, coalesced 64
Index 17: original 68, coalesced 68
Index 18: original 72, coalesced 72
Index 19: original 76, coalesced 76
Index 20: original 80, coalesced 80
Index 21: original 84, coalesced 84
Index 22: original 88, coalesced 88
Index 23: original 92, coalesced 92
Index 24: original 2, coalesced 2
Index 25: original 6, coalesced 6
Index 26: original 10, coalesced 10
Index 27: original 14, coalesced 14
Index 28: original 18, coalesced 18
Index 29: original 22, coalesced 22
Index 30: original 26, coalesced 26
Index 31: original 30, coalesced 30
Index 32: original 34, coalesced 34
Index 33: original 38, coalesced 38
Index 34: original 42, coalesced 42
Index 35: original 46, coalesced 46
Index 36: original 50, coalesced 50
Index 37: original 54, coalesced 54
Index 38: original 58, coalesced 58
Index 39: original 62, coalesced 62
Index 40: original 66, coalesced 66
Index 41: original 70, coalesced 70
Index 42: original 74, coalesced 74
Index 43: original 78, coalesced 78
Index 44: original 82, coalesced 82
Index 45: original 86, coalesced 86
Index 46: original 90, coalesced 90
Index 47: original 94, coalesced 94
Index 48: original 4, coalesced 4
Index 49: original 8, coalesced 8
Index 50: original 12, coalesced 12
Index 51: original 16, coalesced 16
Index 52: original 20, coalesced 20
Index 53: original 24, coalesced 24
Index 54: original 28, coalesced 28
Index 55: original 32, coalesced 32
Index 56: original 36, coalesced 36
Index 57: original 40, coalesced 40
Index 58: original 44, coalesced 44
Index 59: original 48, coalesced 48
Index 60: original 52, coalesced 52
Index 61: original 56, coalesced 56
Index 62: original 60, coalesced 60
Index 63: original 64, coalesced 64
Index 64: original 68, coalesced 68
Index 65: original 72, coalesced 72
Index 66: original 76, coalesced 76
Index 67: original 80, coalesced 80
Index 68: original 84, coalesced 84
Index 69: original 88, coalesced 88
Index 70: original 92, coalesced 92
Index 71: original 96, coalesced 96
Index 72: original 6, coalesced 6
Index 73: original 10, coalesced 10
Index 74: original 14, coalesced 14
Index 75: original 18, coalesced 18
Index 76: original 22, coalesced 22
Index 77: original 26, coalesced 26
Index 78: original 30, coalesced 30
Index 79: original 34, coalesced 34
Index 80: original 38, coalesced 38
Index 81: original 42, coalesced 42
Index 82: original 46, coalesced 46
Index 83: original 50, coalesced 50
Index 84: original 54, coalesced 54
Index 85: original 58, coalesced 58
Index 86: original 62, coalesced 62
Index 87: original 66, coalesced 66
Index 88: original 70, coalesced 70
Index 89: original 74, coalesced 74
Index 90: original 78, coalesced 78
Index 91: original 82, coalesced 82
Index 92: original 86, coalesced 86
Index 93: original 90, coalesced 90
Index 94: original 94, coalesced 94
Index 95: original 98, coalesced 98
Index 96: original 8, coalesced 8
Index 97: original 12, coalesced 12
Index 98: original 16, coalesced 16
Index 99: original 20, coalesced 20
Index 100: original 24, coalesced 24
Index 101: original 28, coalesced 28
Index 102: original 32, coalesced 32
Index 103: original 36, coalesced 36
Index 104: original 40, coalesced 40
Index 105: original 44, coalesced 44
Index 106: original 48, coalesced 48
Index 107: original 52, coalesced 52
Index 108: original 56, coalesced 56
Index 109: original 60, coalesced 60
Index 110: original 64, coalesced 64
Index 111: original 68, coalesced 68
Index 112: original 72, coalesced 72
Index 113: original 76, coalesced 76
Index 114: original 80, coalesced 80
Index 115: original 84, coalesced 84
Index 116: original 88, coalesced 88
Index 117: original 92, coalesced 92
Index 118: original 96, coalesced 96
Index 119: original 100, coalesced 100
Index 120: original 10, coalesced 10
Index 121: original 14, coalesced 14
Index 122: original 18, coalesced 18
Index 123: original 22, coalesced 22
Index 124: original 26, coalesced 26
Index 125: original 30, coalesced 30
Index 126: original 34, coalesced 34
Index 127: original 38, coalesced 38
Index 128: original 42, coalesced 42
Index 129: original 46, coalesced 46
Index 130: original 50, coalesced 50
Index 131: original 54, coalesced 54
Index 132: original 58, coalesced 58
Index 133: original 62, coalesced 62
Index 134: original 66, coalesced 66
Index 135: original 70, coalesced 70
Index 136: original 74, coalesced 74
Index 137: original 78, coalesced 78
Index 138: original 82, coalesced 82
Index 139: original 86, coalesced 86
Index 140: original 90, coalesced 90
Index 141: original 94, coalesced 94
Index 142: original 98, coalesced 98
Index 143: original 102, coalesced 102
By-mode Coalesce Example :
@cute.jit
def bymode_coalesce_example():
    """
    Demonstrates by-mode coalescing
    """
    layout = cute.make_layout((2, (1, 6)), stride=(1, (6, 2)))

    # Coalesce with mode-wise profile (1,1) = coalesce both modes
    result = cute.coalesce(layout, target_profile=(1, 1))
    
    # Print results
    print(">>> Original: ", layout)
    print(">>> Coalesced Result: ", result)

bymode_coalesce_example()
>>> Original:  (2,(1,6)):(1,(6,2))
>>> Coalesced Result:  (2,6):(1,2)
2. Composition
Composition of Layout A with Layout B creates a new layout R = A ◦ B where:

The shape of B is compatible with the shape of R so that all coordinates of B can also be used as coordinates of R,
R(c) = A(B(c)) for all coordinates c in B's domain.
Layout composition is very useful for reshaping and reordering layouts.

Examples
Basic Composition Example :
@cute.jit
def composition_example():
    """
    Demonstrates basic layout composition R = A ◦ B
    """
    A = cute.make_layout((6, 2), stride=(cutlass.Int32(8), 2)) # Dynamic stride
    B = cute.make_layout((4, 3), stride=(3, 1))
    R = cute.composition(A, B)

    # Print static and dynamic information
    print(">>> Layout A:", A)
    cute.printf(">?? Layout A: {}", A)
    print(">>> Layout B:", B) 
    cute.printf(">?? Layout B: {}", B)
    print(">>> Composition R = A ◦ B:", R)
    cute.printf(">?? Composition R: {}", R)

composition_example()
>>> Layout A: (6,2):(?,2)
>>> Layout B: (4,3):(3,1)
>>> Composition R = A ◦ B: ((2,2),3):((?{div=3},2),?)
>?? Layout A: (6,2):(8,2)
>?? Layout B: (4,3):(3,1)
>?? Composition R: ((2,2),3):((24,2),8)
Comparing Composition with static and dynamic layouts :
In this case, the results may look different but are mathematically the same. The 1s in the shape don't affect the layout as a mathematical function on the integers. In the dynamic case, CuTe can not coalesce the dynamic size-1 modes to "simplify" the layout because it is not valid to do so for all possible dynamic values that parameter could realize at runtime.

@cute.jit
def composition_static_vs_dynamic_layout():
    """
    Shows difference between static and dynamic composition results
    """
    # Static version - using compile-time values
    A_static = cute.make_layout(
        (10, 2), 
        stride=(16, 4)
    )
    B_static = cute.make_layout(
        (5, 4), 
        stride=(1, 5)
    )
    R_static = cute.composition(A_static, B_static)

    # Static print shows compile-time info
    print(">>> Static composition:")
    print(">>> A_static: ", A_static)
    print(">>> B_static: ", B_static)
    print(">>> R_static: ", R_static)

    # Dynamic version - using runtime Int32 values
    A_dynamic = cute.make_layout(
        (cutlass.Int32(10), cutlass.Int32(2)),
        stride=(cutlass.Int32(16), cutlass.Int32(4))
    )
    B_dynamic = cute.make_layout(
        (cutlass.Int32(5), cutlass.Int32(4)),
        stride=(cutlass.Int32(1), cutlass.Int32(5))
    )
    R_dynamic = cute.composition(A_dynamic, B_dynamic)
    
    # Dynamic printf shows runtime values
    cute.printf(">?? Dynamic composition:")
    cute.printf(">?? A_dynamic: {}", A_dynamic)
    cute.printf(">?? B_dynamic: {}", B_dynamic)
    cute.printf(">?? R_dynamic: {}", R_dynamic)

composition_static_vs_dynamic_layout()
>>> Static composition:
>>> A_static:  (10,2):(16,4)
>>> B_static:  (5,4):(1,5)
>>> R_static:  (5,(2,2)):(16,(80,4))
>?? Dynamic composition:
>?? A_dynamic: (10,2):(16,4)
>?? B_dynamic: (5,4):(1,5)
>?? R_dynamic: ((5,1),(2,2)):((16,4),(80,4))
By-mode Composition Example :
By-mode composition allows us to apply composition operations to individual modes of a layout. This is particularly useful when you want to manipulate specific modes layout independently (e.g. rows and columns).

In the context of CuTe, by-mode composition is achieved by using a Tiler, which can be a layout or a tuple of layouts. The leaves of the Tiler tuple specify how the corresponding mode of the target layout should be composed, allowing for sublayouts to be treated independently.

@cute.jit
def bymode_composition_example():
    """
    Demonstrates by-mode composition using a tiler
    """
    # Define the original layout A
    A = cute.make_layout(
        (cutlass.Int32(12), (cutlass.Int32(4), cutlass.Int32(8))), 
        stride=(cutlass.Int32(59), (cutlass.Int32(13), cutlass.Int32(1)))
    )

    # Define the tiler for by-mode composition
    tiler = (3, 8) # Apply 3:1 to mode-0 and 8:1 to mode-1

    # Apply by-mode composition
    result = cute.composition(A, tiler)

    # Print static and dynamic information
    print(">>> Layout A:", A)
    cute.printf(">?? Layout A: {}", A)
    print(">>> Tiler:", tiler)
    cute.printf(">?? Tiler: {}", tiler)
    print(">>> By-mode Composition Result:", result)
    cute.printf(">?? By-mode Composition Result: {}", result)

bymode_composition_example()
>>> Layout A: (?,(?,?)):(?,(?,?))
>>> Tiler: (3, 8)
>>> By-mode Composition Result: (3,(?,?)):(?,(?,?))
>?? Layout A: (12,(4,8)):(59,(13,1))
>?? Tiler: (3,8)
>?? By-mode Composition Result: (3,(4,2)):(59,(13,1))
3. Division (Splitting into Tiles)
The Division operation in CuTe is used to split a layout into tiles, which is particularly useful for partitioning data across threads or memory hierarchies.

Examples :
Logical divide :
When applied to two Layouts, logical_divide splits a layout into two modes -- the first mode contains the elements pointed to by the tiler, and the second mode contains the remaining elements.

@cute.jit
def logical_divide_1d_example():
    """
    Demonstrates 1D logical divide
    """
    # Define the original layout
    layout = cute.make_layout((4, 2, 3), stride=(2, 1, 8))  # (4,2,3):(2,1,8)
    
    # Define the tiler
    tiler = cute.make_layout(4, stride=2)  # Apply to layout 4:2
    
    # Apply logical divide
    result = cute.logical_divide(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Logical Divide Result:", result)
    cute.printf(">?? Logical Divide Result: {}", result)

logical_divide_1d_example()
>>> Layout: (4,2,3):(2,1,8)
>>> Tiler : 4:2
>>> Logical Divide Result: ((2,2),(2,3)):((4,1),(2,8))
>?? Logical Divide Result: ((2,2),(2,3)):((4,1),(2,8))
When applied to a Layout and a Tiler tuple, logical_divide applies itself to the leaves of the Tilerand the corresponding mode of the target Layout. This means that the sublayouts are split independently according to the layouts within the Tiler.

@cute.jit
def logical_divide_2d_example():
    """
    Demonstrates 2D logical divide :
    Layout Shape : (M, N, L, ...)
    Tiler Shape  : <TileM, TileN>
    Result Shape : ((TileM,RestM), (TileN,RestN), L, ...)
    """
    # Define the original layout
    layout = cute.make_layout((9, (4, 8)), stride=(59, (13, 1)))  # (9,(4,8)):(59,(13,1))
    
    # Define the tiler
    tiler = (cute.make_layout(3, stride=3),            # Apply to mode-0 layout 3:3
             cute.make_layout((2, 4), stride=(1, 8)))  # Apply to mode-1 layout (2,4):(1,8)
    
    # Apply logical divide
    result = cute.logical_divide(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Logical Divide Result:", result)
    cute.printf(">?? Logical Divide Result: {}", result)

logical_divide_2d_example()
>>> Layout: (9,(4,8)):(59,(13,1))
>>> Tiler : (<cutlass.cute.core._Layout object at 0x7fc95a4ca7b0>, <cutlass.cute.core._Layout object at 0x7fc958160f50>)
>>> Logical Divide Result: ((3,3),((2,4),(2,2))):((177,59),((13,2),(26,1)))
>?? Logical Divide Result: ((3,3),((2,4),(2,2))):((177,59),((13,2),(26,1)))
Zipped, tiled, and flat divide are flavors of logical_divide that potentially rearrange modes into more convenient forms.

Zipped Divide :
@cute.jit
def zipped_divide_example():
    """
    Demonstrates zipped divide :
    Layout Shape : (M, N, L, ...)
    Tiler Shape  : <TileM, TileN>
    Result Shape : ((TileM,TileN), (RestM,RestN,L,...))
    """
    # Define the original layout
    layout = cute.make_layout((9, (4, 8)), stride=(59, (13, 1)))  # (9,(4,8)):(59,(13,1))
    
    # Define the tiler
    tiler = (cute.make_layout(3, stride=3),            # Apply to mode-0 layout 3:3
             cute.make_layout((2, 4), stride=(1, 8)))  # Apply to mode-1 layout (2,4):(1,8)
    
    # Apply zipped divide
    result = cute.zipped_divide(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Zipped Divide Result:", result)
    cute.printf(">?? Zipped Divide Result: {}", result)

zipped_divide_example()
>>> Layout: (9,(4,8)):(59,(13,1))
>>> Tiler : (<cutlass.cute.core._Layout object at 0x7fc95a4ca7b0>, <cutlass.cute.core._Layout object at 0x7fc9581611f0>)
>>> Zipped Divide Result: ((3,(2,4)),(3,(2,2))):((177,(13,2)),(59,(26,1)))
>?? Zipped Divide Result: ((3,(2,4)),(3,(2,2))):((177,(13,2)),(59,(26,1)))
Tiled Divide :
@cute.jit
def tiled_divide_example():
    """
    Demonstrates tiled divide :
    Layout Shape : (M, N, L, ...)
    Tiler Shape  : <TileM, TileN>
    Result Shape : ((TileM,TileN), RestM, RestN, L, ...)
    """
    # Define the original layout
    layout = cute.make_layout((9, (4, 8)), stride=(59, (13, 1)))  # (9,(4,8)):(59,(13,1))
    
    # Define the tiler
    tiler = (cute.make_layout(3, stride=3),            # Apply to mode-0 layout 3:3
             cute.make_layout((2, 4), stride=(1, 8)))  # Apply to mode-1 layout (2,4):(1,8)
    
    # Apply tiled divide
    result = cute.tiled_divide(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Tiled Divide Result:", result)
    cute.printf(">?? Tiled Divide Result: {}", result)

tiled_divide_example()
>>> Layout: (9,(4,8)):(59,(13,1))
>>> Tiler : (<cutlass.cute.core._Layout object at 0x7fc9581610d0>, <cutlass.cute.core._Layout object at 0x7fc958161070>)
>>> Tiled Divide Result: ((3,(2,4)),3,(2,2)):((177,(13,2)),59,(26,1))
>?? Tiled Divide Result: ((3,(2,4)),3,(2,2)):((177,(13,2)),59,(26,1))
Flat Divide :
@cute.jit
def flat_divide_example():
    """
    Demonstrates flat divide :
    Layout Shape : (M, N, L, ...)
    Tiler Shape  : <TileM, TileN>
    Result Shape : (TileM, TileN, RestM, RestN, L, ...)
    """
    # Define the original layout
    layout = cute.make_layout((9, (4, 8)), stride=(59, (13, 1)))  # (9,(4,8)):(59,(13,1))
    
    # Define the tiler
    tiler = (cute.make_layout(3, stride=3),            # Apply to mode-0 layout 3:3
             cute.make_layout((2, 4), stride=(1, 8)))  # Apply to mode-1 layout (2,4):(1,8)
    
    # Apply flat divide
    result = cute.flat_divide(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Flat Divide Result:", result)
    cute.printf(">?? Flat Divide Result: {}", result)

flat_divide_example()
>>> Layout: (9,(4,8)):(59,(13,1))
>>> Tiler : (<cutlass.cute.core._Layout object at 0x7fc958161430>, <cutlass.cute.core._Layout object at 0x7fc9581610d0>)
>>> Flat Divide Result: (3,(2,4),3,(2,2)):(177,(13,2),59,(26,1))
>?? Flat Divide Result: (3,(2,4),3,(2,2)):(177,(13,2),59,(26,1))
4. Product (Reproducing a Tile)
The Product operation in CuTe is used to reproduce one layout according to another layout. It creates a new layout where:

The first mode is the original layout A.
The second mode is a restrided layout B that points to the origin of a "unique replication" of A.
This is particularly useful for repeating layouts of threads across a tile of data for creating "repeat" patterns.

Examples
Logical Product :
@cute.jit
def logical_product_1d_example():
    """
    Demonstrates 1D logical product
    """
    # Define the original layout
    layout = cute.make_layout((2, 2), stride=(4, 1))  # (2,2):(4,1)
    
    # Define the tiler
    tiler = cute.make_layout(6, stride=1)  # Apply to layout 6:1
    
    # Apply logical product
    result = cute.logical_product(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Logical Product Result:", result)
    cute.printf(">?? Logical Product Result: {}", result)

logical_product_1d_example()
>>> Layout: (2,2):(4,1)
>>> Tiler : 6:1
>>> Logical Product Result: ((2,2),(2,3)):((4,1),(2,8))
>?? Logical Product Result: ((2,2),(2,3)):((4,1),(2,8))
Blocked and Raked Product :

Blocked Product: Combines the modes of A and B in a block-like fashion, preserving the semantic meaning of the modes by reassociating them after the product.
Raked Product: Combines the modes of A and B in an interleaved or "raked" fashion, creating a cyclic distribution of the tiles.
@cute.jit
def blocked_raked_product_example():
    """
    Demonstrates blocked and raked products
    """
    # Define the original layout
    layout = cute.make_layout((2, 5), stride=(5, 1))
    
    # Define the tiler
    tiler = cute.make_layout((3, 4), stride=(1, 3))
    
    # Apply blocked product
    blocked_result = cute.blocked_product(layout, tiler=tiler)

    # Apply raked product
    raked_result = cute.raked_product(layout, tiler=tiler)
    
    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Blocked Product Result:", blocked_result)
    print(">>> Raked Product Result:", raked_result)
    cute.printf(">?? Blocked Product Result: {}", blocked_result)
    cute.printf(">?? Raked Product Result: {}", raked_result)

blocked_raked_product_example()
>>> Layout: (2,5):(5,1)
>>> Tiler : (3,4):(1,3)
>>> Blocked Product Result: ((2,3),(5,4)):((5,10),(1,30))
>>> Raked Product Result: ((3,2),(4,5)):((10,5),(30,1))
>?? Blocked Product Result: ((2,3),(5,4)):((5,10),(1,30))
>?? Raked Product Result: ((3,2),(4,5)):((10,5),(30,1))
Zipped, tiled, and flat product :

Similar to divide operations, zipped, tiled, and flat product are flavors of logical_product that potentially rearrange modes into more convenient forms.
@cute.jit
def zipped_tiled_flat_product_example():
    """
    Demonstrates zipped, tiled, and flat products
    Layout Shape : (M, N, L, ...)
    Tiler Shape  : <TileM, TileN>

    zipped_product  : ((M,N), (TileM,TileN,L,...))
    tiled_product   : ((M,N), TileM, TileN, L, ...)
    flat_product    : (M, N, TileM, TileN, L, ...)
    """
    # Define the original layout
    layout = cute.make_layout((2, 5), stride=(5, 1))
    
    # Define the tiler
    tiler = cute.make_layout((3, 4), stride=(1, 3))

    # Apply zipped product
    zipped_result = cute.zipped_product(layout, tiler=tiler)
    
    # Apply tiled product
    tiled_result = cute.tiled_product(layout, tiler=tiler)
    
    # Apply flat product
    flat_result = cute.flat_product(layout, tiler=tiler)

    # Print results
    print(">>> Layout:", layout)
    print(">>> Tiler :", tiler)
    print(">>> Zipped Product Result:", zipped_result)
    print(">>> Tiled Product Result:", tiled_result)
    print(">>> Flat Product Result:", flat_result)
    cute.printf(">?? Zipped Product Result: {}", zipped_result)
    cute.printf(">?? Tiled Product Result: {}", tiled_result)
    cute.printf(">?? Flat Product Result: {}", flat_result)

zipped_tiled_flat_product_example()
>>> Layout: (2,5):(5,1)
>>> Tiler : (3,4):(1,3)
>>> Zipped Product Result: ((2,5),(3,4)):((5,1),(10,30))
>>> Tiled Product Result: ((2,5),3,4):((5,1),10,30)
>>> Flat Product Result: (2,5,3,4):(5,1,10,30)
>?? Zipped Product Result: ((2,5),(3,4)):((5,1),(10,30))
>?? Tiled Product Result: ((2,5),3,4):((5,1),10,30)
>?? Flat Product Result: (2,5,3,4):(5,1,10,30)

==================================================================================================

import torch
from functools import partial

import cutlass
import cutlass.cute as cute
from cutlass.cute.runtime import from_dlpack
Tutorial: Elementwise Add Kernel in CuTe DSL
This tutorial demonstrates how to implement a simple elementwise addition kernel using the CuTe DSL (Domain Specific Language).

Elementwise Addition
Elementwise addition is a fundamental operation in linear algebra. Given two tensors of the same shape, the operation performs element-wise addition to produce a result tensor of the same shape.

For two 2D tensors :math:A and :math:B of shape :math:(M, N), the elementwise addition operation :math:C = A + B is defined as:


where:

 represents the row index
 represents the column index
, 
, and 
 are the elements at position 
 in tensors 
, 
, and 
 respectively
This operation is performed independently for each element position, making it highly parallelizable and well-suited for GPU implementation.

Naive Elementwise Add Kernel
Let's start with a naive implementation that loads each element from 
 and 
, adds them, and stores the result back to 
.

@cute.kernel
def naive_elementwise_add_kernel(
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()
    bdim, _, _ = cute.arch.block_dim()

    thread_idx = bidx * bdim + tidx

    # Map thread index to logical index of input tensor
    m, n = gA.shape
    ni = thread_idx % n
    mi = thread_idx // n

    # Map logical index to physical address via tensor layout
    a_val = gA[mi, ni]
    b_val = gB[mi, ni]

    # Perform element-wise addition
    gC[mi, ni] = a_val + b_val
Structure of the Kernel
The naive kernel simply maps each thread to one element with a 1-to-1 mapping. In this kernel, we don't use CuTe layout algebra but only use basic addressing to index the tensor.

We can launch the kernel with the following JIT function:

@cute.jit
def naive_elementwise_add(
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor
):
    num_threads_per_block = 256

    m, n = mA.shape
    kernel = naive_elementwise_add_kernel(mA, mB, mC)
    kernel.launch(grid=((m * n) // num_threads_per_block, 1, 1),
                  block=(num_threads_per_block, 1, 1))

M, N = 2048, 2048

a = torch.randn(M, N, device="cuda", dtype=torch.float16)
b = torch.randn(M, N, device="cuda", dtype=torch.float16)
c = torch.zeros(M, N, device="cuda", dtype=torch.float16)

a_ = from_dlpack(a, assumed_align=16)
b_ = from_dlpack(b, assumed_align=16)
c_ = from_dlpack(c, assumed_align=16)

# Compile kernel
naive_elementwise_add_ = cute.compile(naive_elementwise_add, a_, b_, c_)
naive_elementwise_add_(a_, b_, c_)

# verify correctness
torch.testing.assert_close(c, a + b)
Benchmark performance
Here's a utility function to benchmark our kernel implementations:

def benchmark(callable, *, num_warmups, num_iterations):
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    torch.cuda.synchronize()

    for _ in range(num_warmups):
        callable()

    start_event.record(stream=torch.cuda.current_stream())
    for _ in range(num_iterations):
        callable()
    end_event.record(stream=torch.cuda.current_stream())
    torch.cuda.synchronize()

    elapsed_time = start_event.elapsed_time(end_event)
    avg_time = elapsed_time / num_iterations

    print(f"Average execution time: {avg_time:.4f} ms")
    print(f"Throughput: {(3 * a.numel() * 2) / (avg_time / 1000) / 1e9:.2f} GB/s")
benchmark(partial(naive_elementwise_add_, a_, b_, c_), num_warmups=5, num_iterations=100)
Average execution time: 0.0385 ms
Throughput: 653.44 GB/s
Performance Analysis
While our naive implementation maps thread indices to contiguous tensor dimensions for coalesced memory access, it doesn't have enough in-flight load & store operations to hide memory latency.

According to Little's Law:


Where:

 is the average number of items in a system
 is the average arrival rate of items (bandwidth)
 is the average time an item spends in the system (latency)
For our elementwise addition kernel:

: The number of load & store operations in-flight
 (Bandwidth): Data transfer rate between memory and compute units
 (Latency): Round-trip delay of memory requests
For memory-bound operations like elementwise addition, performance is limited by the number of in-flight load & store operations.

Vectorized Load and Store
To improve performance according to Little's Law, we need to increase the number of in-flight requests. We can do this by increasing the number of bytes handled in each load & store operation per thread through vectorized memory access.

Since Ampere GPUs support up to 128-bit per load/store and each element is 32-bit, we can load 4 elements per vectorized operation on contiguous rows. CuTe tiling operations make this vectorization straightforward.

Using tiled_tensor = cute.zipped_divide(tensor, tiler), we can partition the input tensor into groups of tiler blocks. For vectorization, we specify tiler as the block of data each thread accesses (4 contiguous elements in the same row, or (1,4)). Different threads can then access different blocks by indexing into the 2nd mode of tiled_tensor.

mA : cute.Tensor                           # (2048,2048):(2048,1)
gA = cute.zipped_divide(a, tiler=(1, 4))   # tiled/vectorized => ((1,4),(2048,512)):((0,1),(2048,4))
 
 
 		 
 
 
 	
 

@cute.kernel
def vectorized_elementwise_add_kernel(
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()
    bdim, _, _ = cute.arch.block_dim()

    thread_idx = bidx * bdim + tidx

    # Map thread index to logical index of input tensor
    m, n = gA.shape[1]       # thread-domain
    ni = thread_idx % n
    mi = thread_idx // n

    # Map logical index to physical address via tensor layout
    a_val = gA[(None, (mi, ni))].load()
    b_val = gB[(None, (mi, ni))].load()
    print(f"[DSL INFO] sliced gA = {gA[(None, (mi, ni))]}")
    print(f"[DSL INFO] sliced gB = {gB[(None, (mi, ni))]}")

    # Perform element-wise addition
    gC[(None, (mi, ni))] = a_val + b_val
This vectorized kernel follows a similar structure to its naive non-vectorized counterpart, with one key difference: the tensor slicing pattern. By using (None, (mi, ni)) as the slice indices, we can extract a (1,4) sub-tensor from gA, gB and gC like

gA[(None, (mi, ni))]
Then tensor data can be loaded into vector via the .load() method.

                                         slice
    ((1,4),(2048,512)):((0,1),(2048,4))   ==>  ((1,4)):((0,1))
       ^     ^    ^
       |     |    |
     (None, (mi,  ni))
@cute.jit
def vectorized_elementwise_add(
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor
):
    threads_per_block = 256

    gA = cute.zipped_divide(mA, (1, 4))
    gB = cute.zipped_divide(mB, (1, 4))
    gC = cute.zipped_divide(mC, (1, 4))

    print(f"[DSL INFO] Tiled Tensors:")
    print(f"[DSL INFO]   gA = {gA}")
    print(f"[DSL INFO]   gB = {gB}")
    print(f"[DSL INFO]   gC = {gC}")

    vectorized_elementwise_add_kernel(gA, gB, gC).launch(
        grid=(cute.size(gC, mode=[1]) // threads_per_block, 1, 1),
        block=(threads_per_block, 1, 1),
    )

a = torch.randn(M, N, device="cuda", dtype=torch.float16)
b = torch.randn(M, N, device="cuda", dtype=torch.float16)
c = torch.zeros(M, N, device="cuda", dtype=torch.float16)

a_ = from_dlpack(a, assumed_align=16)
b_ = from_dlpack(b, assumed_align=16)
c_ = from_dlpack(c, assumed_align=16)

compiled_func = cute.compile(vectorized_elementwise_add, a_, b_, c_)
compiled_func(a_, b_, c_)

# verify correctness
torch.testing.assert_close(c, a + b)
[DSL INFO] Tiled Tensors:
[DSL INFO]   gA = tensor<ptr<f16, gmem, align<16>> o ((1,4),(2048,512)):((0,1),(2048,4))>
[DSL INFO]   gB = tensor<ptr<f16, gmem, align<16>> o ((1,4),(2048,512)):((0,1),(2048,4))>
[DSL INFO]   gC = tensor<ptr<f16, gmem, align<16>> o ((1,4),(2048,512)):((0,1),(2048,4))>
[DSL INFO] sliced gA = tensor<ptr<f16, gmem, align<8>> o ((1,4)):((0,1))>
[DSL INFO] sliced gB = tensor<ptr<f16, gmem, align<8>> o ((1,4)):((0,1))>
benchmark(partial(compiled_func, a_, b_, c_), num_warmups=5, num_iterations=100)
Average execution time: 0.0202 ms
Throughput: 1244.98 GB/s
TV Layout
Both the naive and vectorized kernels follow a common pattern to map thread indices to physical addresses:

Step 1: Map thread index to logical M/N coordinates

    mi = thread_idx // n
    ni = thread_idx % n
Step 2: Map logical M/N coordinates to physical addresses using the tensor layout

    a[(None, (mi, ni))].load()
CuTe uses TV layout to represent this mapping from thread index and value index (i.e., the 4 elements loaded per thread) to the logical coordinate space of a tensor. By configuring different TV layouts, we can experiment with different memory access patterns with minimal code changes.

The following example demonstrates two levels of tiling: at the thread-block level and at the thread level.

For thread-block level tiling, each input & output tensor is first divided into a group of (TileM, TileN) sub-tensors at the host side.

Inside the GPU kernel, we provide the thread-block index to the 2nd mode of the tiled tensor (gA[((None, None), bidx)]), which returns a thread-block local view of a single (TileM, TileN) sub-tensor.

For thread level tiling, we compose the sub-tensor (which maps from logical coordinates to physical addresses) with the TV layout (which maps from thread & value indices to logical coordinates). This gives us a tiled sub-tensor that maps from thread & value indices directly to physical addresses.

We then provide the thread index to the tiled sub-tensor (tidfrgA[(tidx, None)]) to get a thread-local view of the data each thread accesses. Note that the thread index is now in the 1st mode, as the tiled sub-tensor puts the thread mode before the value mode.

@cute.kernel
def elementwise_add_kernel(
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
    tv_layout: cute.Layout
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()

    #--------------------------------
    # slice for thread-block level view
    #--------------------------------
    blk_coord = ((None, None), bidx)

    # logical coord -> address
    blkA = gA[blk_coord]  # (TileM, TileN) -> physical address
    blkB = gB[blk_coord]  # (TileM, TileN) -> physical address
    blkC = gC[blk_coord]  # (TileM, TileN) -> physical address

    #--------------------------------
    # compose for thread-index & value-index to physical mapping
    #--------------------------------
    # blockA:    (TileM, TileN) -> physical address
    # tv_layout: (tid, vid)     -> (TileM, TileN)
    # tidfrgA = blkA o tv_layout
    # tidfrgA:   (tid, vid) -> physical address
    tidfrgA = cute.composition(blkA, tv_layout)
    tidfrgB = cute.composition(blkB, tv_layout)
    tidfrgC = cute.composition(blkC, tv_layout)

    print(f"Composed with TV layout:")
    print(f"  tidfrgA: {tidfrgA.type}")

    #--------------------------------
    # slice for thread-level view
    #--------------------------------
    # `None` represent slice of the entire per-thread data
    thr_coord = (tidx, None)

    # slice for threads: vid -> address
    thrA = tidfrgA[thr_coord]  # (V) -> physical address
    thrB = tidfrgB[thr_coord]  # (V) -> physical address
    thrC = tidfrgC[thr_coord]  # (V) -> physical address

    thrC[None] = thrA.load() + thrB.load()
If we take a closer look at the layout of zipped divided input tensor gA:

Tiled to Thread Block:

    ((16,256),(128,8))  : ((2048,1),(32768,256))
     ~~~~~~~~  ~~~~~~      ~~~~~~~~
        |        |            |
        |        |            |
        |        `------------------------> Number of Thread Blocks
        |                     |
        |                     |
        `--------------------'
                  |
                  V
             Thread Block
                 Tile

Sliced to Thread-Block local sub-tensor (a (16, 256) tile):  gA[((None, None), bidx)]

    (16,256)   :  (2048,1)
     ~~~~~~        ~~~~~~
        |             |        Tiled/Composed with TV Layout
        |             |    
        |             |    o   ((32,4),(8,4)):((128,4),(16,1))
        V             V         
~~~~~~~~~~~~~~~     ~~~~~~~~~~~~~~~~~~~ 
((32,4), (8,4))  :  ((4,8192),(1,2048))
    |      |
    |      `--------> per thread fragment
    |
Thread Block
  Shape

Sliced to Thread local sub-tensor (a (4,8) tile):  tidfrgA[(tidx, None)]

The host code below shows the construction of the TV layout. By composing a thread layout of (4,32):(32,1) (32 threads read contiguous elements on the row dimension, then 4 warps read different rows) with a value layout of (4,8):(8,1) (each thread reads 8 contiguous elements on the row dimension across 4 contiguous rows), we obtain the TV layout shown in the figure above.

@cute.jit
def elementwise_add(
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor,
):
    # mA layout: (M, N):(N, 1)
    # TV layout map thread & value index to (16, 256) logical tile
    #  - contiguous thread index maps to mode-1 because input layout is contiguous on
    #     mode-1 for coalesced load-store
    #  - each thread load 8 contiguous element each row and load 4 rows
    thr_layout = cute.make_layout((4, 32), stride=(32, 1))
    val_layout = cute.make_layout((4, 8), stride=(8, 1))
    tiler_mn, tv_layout = cute.make_layout_tv(thr_layout, val_layout)
    print(f"Tiler: {tiler_mn}")
    print(f"TV Layout: {tv_layout}")

    gA = cute.zipped_divide(mA, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gB = cute.zipped_divide(mB, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gC = cute.zipped_divide(mC, tiler_mn)  # ((TileM, TileN), (RestM, RestN))

    print(f"Tiled Input Tensors:")
    print(f"  gA: {gA.type}")
    print(f"  gB: {gB.type}")
    print(f"  gC: {gC.type}")

    # Launch the kernel asynchronously
    # Async token(s) can also be specified as dependencies
    elementwise_add_kernel(
        gA, gB, gC, tv_layout
    ).launch(
        grid=[cute.size(gC, mode=[1]), 1, 1],
        block=[cute.size(tv_layout, mode=[0]), 1, 1],
    )

a = torch.randn(M, N, device="cuda", dtype=torch.float16)
b = torch.randn(M, N, device="cuda", dtype=torch.float16)
c = torch.zeros(M, N, device="cuda", dtype=torch.float16)

a_ = from_dlpack(a, assumed_align=16)
b_ = from_dlpack(b, assumed_align=16)
c_ = from_dlpack(c, assumed_align=16)

elementwise_add_ = cute.compile(elementwise_add, a_, b_, c_)
elementwise_add_(a_, b_, c_)

# verify correctness
torch.testing.assert_close(c, a + b)
Tiler: (16, 256)
TV Layout: ((32,4),(8,4)):((128,4),(16,1))
Tiled Input Tensors:
  gA: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gB: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gC: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
Composed with TV layout:
  tidfrgA: !cute.memref<f16, gmem, align<16>, "((32,4),(8,4)):((8,8192),(1,2048))">
benchmark(partial(elementwise_add_, a_, b_, c_), num_warmups=5, num_iterations=200)
Average execution time: 0.0222 ms
Throughput: 1133.58 GB/s
Using Lambda Function
CuTe DSL is built on top of Python. It can leverage Python to implement meta-programming to generate flexible kernels. E.g. we can write kernel template that take custom binary operations to generate kernels for arbitrary binary operations.

@cute.jit
def elementwise_apply(
    op: cutlass.Constexpr,
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor
):
    ...
@cute.kernel
def elementwise_apply_kernel(
    op: cutlass.Constexpr,    # lambda function must be const expr to generate code at compile time
    gA: cute.Tensor,
    gB: cute.Tensor,
    gC: cute.Tensor,
    tv_layout: cute.Layout
):
    tidx, _, _ = cute.arch.thread_idx()
    bidx, _, _ = cute.arch.block_idx()

    blk_coord = ((None, None), bidx)

    # logical coord -> address
    blkA = gA[blk_coord]  # (TileM, TileN) -> physical address
    blkB = gB[blk_coord]  # (TileM, TileN) -> physical address
    blkC = gC[blk_coord]  # (TileM, TileN) -> physical address

    tidfrgA = cute.composition(blkA, tv_layout)
    tidfrgB = cute.composition(blkB, tv_layout)
    tidfrgC = cute.composition(blkC, tv_layout)

    print(f"Composed with TV layout:")
    print(f"  tidfrgA: {tidfrgA.type}")

    thr_coord = (tidx, None)

    # slice for threads: vid -> address
    thrA = tidfrgA[thr_coord]  # (V) -> physical address
    thrB = tidfrgB[thr_coord]  # (V) -> physical address
    thrC = tidfrgC[thr_coord]  # (V) -> physical address

    #--------------------------------
    # apply custom operation
    #--------------------------------
    thrC[None] = op(thrA.load(), thrB.load())


@cute.jit
def elementwise_op(
    op: cutlass.Constexpr,
    mA: cute.Tensor,
    mB: cute.Tensor,
    mC: cute.Tensor,
):
    # mA layout: (M, N):(N, 1)
    # TV layout map thread & value index to (16, 256) logical tile
    #  - contiguous thread index maps to mode-1 because input layout is contiguous on
    #     mode-1 for coalesced load-store
    #  - each thread load 8 contiguous element each row and load 4 rows
    thr_layout = cute.make_layout((4, 32), stride=(32, 1))
    val_layout = cute.make_layout((4, 8), stride=(8, 1))
    tiler_mn, tv_layout = cute.make_layout_tv(thr_layout, val_layout)
    print(f"Tiler: {tiler_mn}")
    print(f"TV Layout: {tv_layout}")

    gA = cute.zipped_divide(mA, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gB = cute.zipped_divide(mB, tiler_mn)  # ((TileM, TileN), (RestM, RestN))
    gC = cute.zipped_divide(mC, tiler_mn)  # ((TileM, TileN), (RestM, RestN))

    print(f"Tiled Input Tensors:")
    print(f"  gA: {gA.type}")
    print(f"  gB: {gB.type}")
    print(f"  gC: {gC.type}")

    # Launch the kernel asynchronously
    # Async token(s) can also be specified as dependencies
    elementwise_apply_kernel(
        op, gA, gB, gC, tv_layout
    ).launch(
        grid=[cute.size(gC, mode=[1]), 1, 1],
        block=[cute.size(tv_layout, mode=[0]), 1, 1],
    )

a = torch.randn(M, N, device="cuda", dtype=torch.float16)
b = torch.randn(M, N, device="cuda", dtype=torch.float16)
c = torch.zeros(M, N, device="cuda", dtype=torch.float16)

a_ = from_dlpack(a, assumed_align=16)
b_ = from_dlpack(b, assumed_align=16)
c_ = from_dlpack(c, assumed_align=16)

from operator import mul

elementwise_op(mul, a_, b_, c_)

# verify correctness
torch.testing.assert_close(c, mul(a, b))
Tiler: (16, 256)
TV Layout: ((32,4),(8,4)):((128,4),(16,1))
Tiled Input Tensors:
  gA: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gB: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gC: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
Composed with TV layout:
  tidfrgA: !cute.memref<f16, gmem, align<16>, "((32,4),(8,4)):((8,8192),(1,2048))">
Custom operators can be more complex. For example, here's a function that performs multiplication followed by ReLU:

def mul_relu(a, b):
    tmp = a * b
    return cute.where(tmp > 0, tmp, cute.full_like(tmp, 0))


# As we uses cute.where in customized operation, we need to create another relu function
def mul_relu_ref(a, b):
    tmp = a * b
    return torch.relu(tmp)


elementwise_op(mul_relu, a_, b_, c_)

# verify correctness
torch.testing.assert_close(c, mul_relu_ref(a, b))
Tiler: (16, 256)
TV Layout: ((32,4),(8,4)):((128,4),(16,1))
Tiled Input Tensors:
  gA: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gB: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
  gC: !cute.memref<f16, gmem, align<16>, "((16,256),(128,8)):((2048,1),(32768,256))">
Composed with TV layout:
  tidfrgA: !cute.memref<f16, gmem, align<16>, "((32,4),(8,4)):((8,8192),(1,2048))">

========================================================================================================

Example 06: CUDA Graphs
In this example we demonstrate how to use CUDA graphs through PyTorch with CuTe DSL. The process of interacting with PyTorch's CUDA graph implementation requires exposing PyTorch's CUDA streams to CUTLASS.

To use CUDA graphs with Blackwell requires a version of PyTorch that supports Blackwell. This can be obtained through:

The PyTorch NGC
PyTorch 2.7 with CUDA 12.8 or later (e.g., pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128)
Building PyTorch directly with your version of CUDA.
# import torch for CUDA graphs
import torch
import cutlass
import cutlass.cute as cute
# import CUstream type from the cuda driver bindings
from cuda.bindings.driver import CUstream
# import the current_stream function from torch
from torch.cuda import current_stream
Kernel Creation
We create a kernel which prints "Hello world" as well as a host function to launch the kernel. We then compile the kernel for use in our graph, by passing in a default stream.

Kernel compilation before graph capture is required since CUDA graphs cannot JIT compile kernels during graph execution.

@cute.kernel
def hello_world_kernel():
    """
    A kernel that prints hello world
    """
    cute.printf("Hello world")

@cute.jit
def hello_world(stream : CUstream):
    """
    Host function that launches our (1,1,1), (1,1,1) grid in stream
    """
    hello_world_kernel().launch(grid=[1, 1, 1], block=[1, 1, 1], stream=stream)

# Grab a stream from PyTorch, this will also initialize our context
# so we can omit cutlass.cuda.initialize_cuda_context()
stream = current_stream()
hello_world_compiled = cute.compile(hello_world, CUstream(stream.cuda_stream))
Creating and replaying a CUDA Graph
We create a stream through torch as well as a graph. When we create the graph we can pass the stream we want to capture to torch. We similarly run the compiled kernel with the stream passed as a CUstream.

Finally we can replay our graph and synchronize.

# Create a CUDA Graph
g = torch.cuda.CUDAGraph()
# Capture our graph
with torch.cuda.graph(g):
    # Turn our torch Stream into a cuStream stream.
    # This is done by getting the underlying CUstream with .cuda_stream
    graph_stream = CUstream(current_stream().cuda_stream)
    # Run 2 iterations of our compiled kernel
    for _ in range(2):
        # Run our kernel in the stream
        hello_world_compiled(graph_stream)

# Replay our graph
g.replay()
# Synchronize all streams (equivalent to cudaDeviceSynchronize() in C++)
torch.cuda.synchronize()
Hello world
Hello world
Our run results in the following execution when viewed in NSight Systems:

Image of two hello world kernels run back to back in a CUDA graph

We can observe the launch of the two kernels followed by a cudaDeviceSynchronize().

Now we can confirm that this minimizes some launch overhead:

# Get our CUDA stream from PyTorch
stream = CUstream(current_stream().cuda_stream)

# Create a larger CUDA Graph of 100 iterations
g = torch.cuda.CUDAGraph()
# Capture our graph
with torch.cuda.graph(g):
    # Turn our torch Stream into a cuStream stream.
    # This is done by getting the underlying CUstream with .cuda_stream
    graph_stream = CUstream(current_stream().cuda_stream)
    # Run 2 iterations of our compiled kernel
    for _ in range(100):
        # Run our kernel in the stream
        hello_world_compiled(graph_stream)

# Create CUDA events for measuring performance
start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)

# Run our kernel to warm up the GPU
for _ in range(100):
    hello_world_compiled(stream)

# Record our start time
start.record()
# Run 100 kernels
for _ in range(100):
    hello_world_compiled(stream)
# Record our end time
end.record()
# Synchronize (cudaDeviceSynchronize())
torch.cuda.synchronize()

# Calculate the time spent when launching kernels in a stream
# Results are in ms
stream_time = start.elapsed_time(end) 

# Warmup our GPU again
g.replay()
# Record our start time
start.record()
# Run our graph
g.replay()
# Record our end time
end.record()
# Synchronize (cudaDeviceSynchronize())
torch.cuda.synchronize()

# Calculate the time spent when launching kernels in a graph
# units are ms
graph_time = start.elapsed_time(end)

# Print out speedup when using CUDA graphs
percent_speedup = (stream_time - graph_time) / graph_time
print(f"{percent_speedup * 100.0:.2f}% speedup when using CUDA graphs for this kernel!")
8.94% speedup when using CUDA graphs for this kernel!